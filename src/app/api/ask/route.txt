// src/app/api/ask/route.ts
// Nerion ASK Route — v3.3 (full-memory always; robust stimulus mapping & scoring)
// - Full short-memory context (Supabase already caps to ~20 latest rows) for Q1 → Q120
// - Oldest→Newest chronology for GPT (conversation flow stays natural)
// - Stimulus→index mapping by exact text to prevent drift
// - Score extraction takes the *last* token ([[SCORE=X]] or {"score": X})
// - No silent overwrite if a qN is already set; set is_complete on q120
// - [AUTO_CONTINUE] respected via system prompt INPUT_MARKERS
// - Image inputs described via analyzeImage() and appended to user content
// - dynamic = force-dynamic to avoid caching surprises

export const runtime = "nodejs";
export const dynamic = "force-dynamic";

import { NextResponse } from "next/server";
import OpenAI from "openai";
import { createClient, SupabaseClient } from "@supabase/supabase-js";
import { buildSystemPrompt } from "../../../lib/prompts/system";
import { storeInShort } from "../../../lib/memory";
import { getBigFiveQuestion, BIG_FIVE_QUESTIONS } from "../../../lib/prompts/big_five";
import { analyzeImage } from "../../../lib/photo";

// ------------ Types ------------
type ImagePart = { type: "image_url"; image_url: { url: string } };
type TextPart = { type: "text"; text: string };
type ChatContentPart = ImagePart | TextPart;
type Msg = OpenAI.Chat.Completions.ChatCompletionMessageParam;

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

// ------------ Helpers ------------
function detectStartIntent(text: string | undefined): boolean {
  if (!text) return false;
  const t = text.toLowerCase();
  return !!t.match(
    /(let'?s\s*(go|begin|start)|\b(start|begin|go)\b|c['’]est parti|on commence|commen[cs]ons|je suis (pr[eé]t|ready)|lance|d[eé]marre|d[eé]butons)/
  );
}

function hasTriggerOrchestrator(s: string | undefined): boolean {
  if (!s) return false;
  return /"trigger_orchestrator"\s*:\s*true/.test(s);
}

function stripTechnicalBlocks(text: string): string {
  return (text ?? "")
    .replace(/```json[\s\S]*?```/g, "")
    .replace(/```[\s\S]*?```/g, "") // drop any fenced blob
    .replace(/\{\s*"trigger_orchestrator"\s*:\s*true\s*\}/g, "")
    .replace(/\{\s*"score"\s*:\s*[1-5]\s*\}/gi, "")
    .replace(/\[\[\s*SCORE\s*=\s*[1-5]\s*\]\]/gi, "")
    .trim();
}

// Take the *last* score token if multiple are produced
function extractScore(s: string): number | null {
  if (!s) return null;
  const re = /\[\[\s*SCORE\s*=\s*([1-5])\s*\]\]|["']?score["']?\s*:\s*([1-5])/gi;
  let m: RegExpExecArray | null;
  let last: string | undefined;
  while ((m = re.exec(s)) !== null) last = (m[1] || m[2]) as string;
  if (!last) return null;
  const n = parseInt(last, 10);
  return Number.isInteger(n) && n >= 1 && n <= 5 ? n : null;
}

async function callOpenAI(
  messages: Msg[],
  model?: string,
  temperature = 0.2,
  max_tokens = 400
) {
  const chosen = model || process.env.NERION_OPENAI_MODEL || "gpt-4";
  const completion = await openai.chat.completions.create({
    model: chosen,
    messages,
    temperature,
    max_tokens,
  });
  return completion.choices?.[0]?.message?.content?.trim() || "";
}

async function ensureBigFiveRow(supabase: SupabaseClient, userId: string) {
  const { data } = await supabase
    .from("big_five")
    .select("user_id")
    .eq("user_id", userId)
    .maybeSingle();
  if (!data) {
    console.log("➕ Création big_five row pour", userId);
    const { error } = await supabase.from("big_five").insert({ user_id: userId });
    if (error) throw error;
  }
}

async function getBigFiveRow(supabase: SupabaseClient, userId: string) {
  const { data, error } = await supabase
    .from("big_five")
    .select("*")
    .eq("user_id", userId)
    .maybeSingle();
  if (error) throw error;
  return (data as any) || null;
}

function findCurrentIndex(row: any): number {
  for (let i = 1; i <= 120; i++) if (row?.[`q${i}`] == null) return i;
  return 121; // tout rempli
}

// Robust: query the last assistant memory that *starts* with [STIMULUS]
async function getLastStimulus(
  supabase: SupabaseClient,
  userId: string
): Promise<string | null> {
  const { data, error } = await supabase
    .from("memories")
    .select("content")
    .eq("user_id", userId)
    .eq("layer", "short")
    .eq("role", "assistant")
    .ilike("content", "[STIMULUS]%")
    .order("created_at", { ascending: false })
    .limit(1);
  if (error || !data || data.length === 0) return null;
  const raw = (data[0] as any).content as string;
  const stim = raw.replace(/^\[STIMULUS\]\s*\n?/, "").trim();
  return stim || null;
}

// Map STIMULUS text back to its canonical question index.
function findIndexByStimulus(stimulus: string): number | null {
  const s = stimulus.trim();
  const match = BIG_FIVE_QUESTIONS.find((q) => q.text === s);
  return match?.index ?? null;
}

// Full short-history (oldest → newest). No explicit cap here; Supabase enforces your table policy.
async function getShortHistoryBounded(
  supabase: SupabaseClient,
  userId: string
): Promise<Msg[]> {
  const { data, error } = await supabase
    .from("memories")
    .select("role, content, created_at")
    .eq("user_id", userId)
    .eq("layer", "short")
    .order("created_at", { ascending: true });
  if (error || !data) return [];
  return (data as any[]).map((m) => ({
    role: m.role === "user" ? "user" : "assistant",
    content: m.content,
  }));
}

// ------------ Route ------------
export async function POST(req: Request) {
  try {
    console.log("🔐 Auth...");
    const authHeader = req.headers.get("authorization") || req.headers.get("Authorization");
    const token = authHeader?.startsWith("Bearer ") ? authHeader.slice(7) : null;
    if (!token) return NextResponse.json({ error: "Unauthorized: no token" }, { status: 401 });

    const supabase = createClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!,
      { auth: { persistSession: false, autoRefreshToken: false } }
    );

    const { data: authData, error: authErr } = await supabase.auth.getUser(token);
    if (authErr || !authData?.user?.id) {
      return NextResponse.json({ error: "Unauthorized: invalid token" }, { status: 401 });
    }
    const userId = authData.user.id;
    console.log("✅ User:", userId);

    await ensureBigFiveRow(supabase, userId);
    const bfRowAtStart = await getBigFiveRow(supabase, userId);
    const isComplete = bfRowAtStart?.is_complete === true;

    // ---- Parse body ----
    const body = await req.json();
    const rawMessage: string | undefined = typeof body.message === "string" ? body.message.trim() : undefined;
    const assistantMessageFromClient: string | undefined =
      typeof body.assistant_message === "string" ? body.assistant_message.trim() : undefined;
    const rawContent: ChatContentPart[] | undefined = Array.isArray(body.content) ? body.content : undefined;

    // Compose user input (texte + image décrite)
    let composedUserInput = "";
    const hasImage = !!rawContent?.some((p) => p.type === "image_url" && (p as ImagePart).image_url?.url);
    const firstImageUrl = rawContent?.find((p): p is ImagePart => p.type === "image_url")?.image_url?.url;

    if (hasImage && firstImageUrl) {
      console.log("🖼️ Image détectée, analyse...");
      const description = await analyzeImage(firstImageUrl);
      composedUserInput = rawMessage ? `${rawMessage}\n\n[Image décrite]\n${description}` : description;
    } else {
      composedUserInput = rawMessage ?? (rawContent as TextPart[])?.map((p) => p.text).join("\n") ?? "";
    }

    console.log("💬 Input user:", composedUserInput || "(vide)");
    if (assistantMessageFromClient) console.log("🤝 assistant_message (client):", assistantMessageFromClient);

    // ---- Stocker le message user si présent ----
    if (composedUserInput) await storeInShort(supabase, userId, "user", composedUserInput);

    // ---- Phase: COMPLETE (test déjà fini) ----
    if (isComplete) {
      console.log("🛑 Test déjà complet → phase 'complete'");
      const sys = buildSystemPrompt({ phase: "complete" });
      const history = await getShortHistoryBounded(supabase, userId);
      const messages: Msg[] = [
        { role: "system", content: sys },
        ...history,
        { role: "user", content: composedUserInput || "[AUTO_CONTINUE]" },
      ];
      const reply = await callOpenAI(messages, undefined, 0.2, 400);
      const cleaned = stripTechnicalBlocks(reply);
      await storeInShort(supabase, userId, "assistant", cleaned || reply);
      return NextResponse.json({ message: cleaned || reply });
    }

    // ---- Détection d'un trigger envoyé depuis le client ----
    if (assistantMessageFromClient && hasTriggerOrchestrator(assistantMessageFromClient)) {
      console.log("🚀 Trigger orchestrator (depuis assistant_message client)");
      const enthusiasm = stripTechnicalBlocks(assistantMessageFromClient);
      if (enthusiasm) await storeInShort(supabase, userId, "assistant", enthusiasm);

      // QN actuelle = première colonne vide
      const bfRow = await getBigFiveRow(supabase, userId);
      let step = findCurrentIndex(bfRow);
      if (step > 120) step = 1;

      const q = getBigFiveQuestion(step);
      const stimulus = q.text;
      await storeInShort(supabase, userId, "assistant", `[STIMULUS]\n${stimulus}`);

      const sys = buildSystemPrompt({ phase: "big_five", stimulus });
      const historyAll: Msg[] = await getShortHistoryBounded(supabase, userId);
      const messages: Msg[] = [{ role: "system", content: sys }, ...historyAll];

      console.log("🤖 GPT start Q", step);
      const firstQ = await callOpenAI(messages, undefined, 0.2, 320);
      await storeInShort(supabase, userId, "assistant", stripTechnicalBlocks(firstQ) || firstQ);

      const out = (enthusiasm ? enthusiasm + "\n\n" : "") + (stripTechnicalBlocks(firstQ) || firstQ);
      return NextResponse.json({ message: out });
    }

    // ---- Détection d'intention de commencer ----
    if (detectStartIntent(composedUserInput)) {
      console.log("✅ Intent de démarrer détecté → phase 'intro' (confirmation intégrée)");
      const introSys = buildSystemPrompt({ phase: "intro" });
      const introMessages: Msg[] = [
        { role: "system", content: introSys },
        { role: "user", content: composedUserInput },
      ];
      const introReply = await callOpenAI(introMessages, undefined, 0.0, 240);
      console.log("💬 INTRO reply:", introReply);
      const enthusiasm = stripTechnicalBlocks(introReply);
      if (enthusiasm) await storeInShort(supabase, userId, "assistant", enthusiasm);

      if (hasTriggerOrchestrator(introReply)) {
        const bfRow = await getBigFiveRow(supabase, userId);
        let step = findCurrentIndex(bfRow);
        if (step > 120) step = 1;

        const q = getBigFiveQuestion(step);
        const stimulus = q.text;
        console.log(`📌 Stimulus q${step}:`, stimulus);
        await storeInShort(supabase, userId, "assistant", `[STIMULUS]\n${stimulus}`);

        const sys = buildSystemPrompt({ phase: "big_five", stimulus });
        const historyAll: Msg[] = await getShortHistoryBounded(supabase, userId);
        const messages: Msg[] = [{ role: "system", content: sys }, ...historyAll];

        console.log("🤖 GPT start Q", step);
        const firstQ = await callOpenAI(messages, undefined, 0.2, 320);
        const firstQClean = stripTechnicalBlocks(firstQ);
        await storeInShort(supabase, userId, "assistant", firstQClean || firstQ);

        const out = (enthusiasm ? enthusiasm + "\n\n" : "") + (firstQClean || firstQ);
        return NextResponse.json({ message: out });
      }

      return NextResponse.json({ message: enthusiasm || introReply });
    }

    // ---- Phase Big Five en cours ? (stimulus en attente) ----
    const pendingStimulus = await getLastStimulus(supabase, userId);
    if (pendingStimulus) {
      console.log("🧭 Stimulus en cours détecté → phase 'big_five'");
      const sys = buildSystemPrompt({ phase: "big_five", stimulus: pendingStimulus });
      const history = await getShortHistoryBounded(supabase, userId);
      const messages: Msg[] = [
        { role: "system", content: sys },
        ...history,
        { role: "user", content: composedUserInput || "[AUTO_CONTINUE]" },
      ];

      const assistant = await callOpenAI(messages, undefined, 0.2, 320);
      console.log("💬 BIG5 assistant:", assistant);
      const score = extractScore(assistant);
      const visibleAssistant = stripTechnicalBlocks(assistant);
      if (visibleAssistant) await storeInShort(supabase, userId, "assistant", visibleAssistant);

      // Si pas de score → encore en exploration
      if (score == null) {
        console.log("ℹ️ Pas de SCORE détecté → on renvoie la réponse telle quelle");
        return NextResponse.json({ message: visibleAssistant || assistant });
      }

      console.log("🧠 SCORE détecté:", score);

      const row = await getBigFiveRow(supabase, userId);
      if (!row) {
        console.error("❌ big_five row introuvable pour user:", userId);
        return NextResponse.json({ error: "big_five missing" }, { status: 500 });
      }

      // Prefer mapping by stimulus text to avoid index drift.
      const stimIndex = findIndexByStimulus(pendingStimulus);
      const targetIndex = stimIndex ?? findCurrentIndex(row);
      const qMeta = getBigFiveQuestion(targetIndex);
      const col = qMeta.key;
      const finalScore = qMeta.isReversed ? 6 - score : score;

      if (row[col] != null) {
        console.warn(`⚠️ ${col} already set — skipping overwrite`);
      } else {
        console.log(`📝 Écriture ${col} = ${finalScore} (isReversed=${qMeta.isReversed})`);
        const { error: upErr } = await supabase
          .from("big_five")
          .update({ [col]: finalScore })
          .eq("user_id", userId);
        if (upErr) {
          console.error("❌ Update error:", upErr);
          return NextResponse.json({ error: "DB update error" }, { status: 500 });
        }
      }

      // Fin de test ?
      if (targetIndex === 120) {
        console.log("🏁 Q120 atteinte → is_complete = true");
        const { error: finErr } = await supabase
          .from("big_five")
          .update({ is_complete: true })
          .eq("user_id", userId);
        if (finErr) {
          console.error("❌ Flag complete error:", finErr);
          return NextResponse.json({ error: "DB update error" }, { status: 500 });
        }
        const doneMsg = "✅ Test terminé. Merci pour ta participation.";
        await storeInShort(supabase, userId, "assistant", doneMsg);
        return NextResponse.json({ message: doneMsg });
      }

      // Sinon, préparer Qn+1 et la poser immédiatement
      const nextIndex = targetIndex + 1;
      const nextMeta = getBigFiveQuestion(nextIndex);
      const nextStimulus = nextMeta.text;
      console.log(`➡️ Préparation stimulus q${nextIndex}`);
      await storeInShort(supabase, userId, "assistant", `[STIMULUS]\n${nextStimulus}`);

      const nextSys = buildSystemPrompt({ phase: "big_five", stimulus: nextStimulus });
      const nextHistory = await getShortHistoryBounded(supabase, userId);
      const nextMessages: Msg[] = [
        { role: "system", content: nextSys },
        ...nextHistory,
        { role: "user", content: "[AUTO_CONTINUE]" },
      ];
      console.log("📤 GPT → Q", nextIndex);
      const nextQ = await callOpenAI(nextMessages, undefined, 0.2, 320);
      const nextQClean = stripTechnicalBlocks(nextQ);
      if (nextQClean) await storeInShort(supabase, userId, "assistant", nextQClean);
      return NextResponse.json({ message: nextQClean || nextQ });
    }

    // ---- Phase INTRO par défaut ----
    console.log("👋 Phase 'intro'");
    const sys = buildSystemPrompt({ phase: "intro" });
    const history = await getShortHistoryBounded(supabase, userId);
    const messages: Msg[] = [
      { role: "system", content: sys },
      ...history,
      { role: "user", content: composedUserInput || "[AUTO_CONTINUE]" },
    ];
    const reply = await callOpenAI(messages, undefined, 0.2, 280);

    if (hasTriggerOrchestrator(reply)) {
      const enthusiasm = stripTechnicalBlocks(reply);
      if (enthusiasm) await storeInShort(supabase, userId, "assistant", enthusiasm);

      const bfRow = await getBigFiveRow(supabase, userId);
      let step = findCurrentIndex(bfRow);
      if (step > 120) step = 1;

      const q = getBigFiveQuestion(step);
      const stimulus = q.text;
      console.log(`📌 Stimulus q${step}:`, stimulus);
      await storeInShort(supabase, userId, "assistant", `[STIMULUS]\n${stimulus}`);

      const qSys = buildSystemPrompt({ phase: "big_five", stimulus });
      const historyAll: Msg[] = await getShortHistoryBounded(supabase, userId);
      const qMessages: Msg[] = [{ role: "system", content: qSys }, ...historyAll];

      console.log("🤖 GPT start Q", step);
      const firstQ = await callOpenAI(qMessages, undefined, 0.2, 320);
      const firstQClean = stripTechnicalBlocks(firstQ);
      await storeInShort(supabase, userId, "assistant", firstQClean || firstQ);

      const out = (enthusiasm ? enthusiasm + "\n\n" : "") + (firstQClean || firstQ);
      return NextResponse.json({ message: out });
    }

    const cleaned = stripTechnicalBlocks(reply);
    await storeInShort(supabase, userId, "assistant", cleaned || reply);
    return NextResponse.json({ message: cleaned || reply });
  } catch (err) {
    console.error("❌ Error in ask route:", err);
    return NextResponse.json({ error: "Server error" }, { status: 500 });
  }
}
