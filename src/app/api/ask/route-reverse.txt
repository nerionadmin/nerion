// src/app/api/ask/route.ts
// Nerion ASK Route ‚Äî v3 (short-memory only)
// - Intro/confirmation ‚Üí Q1 imm√©diate
// - Big Five: scoring auto + encha√Ænement Qn+1
// - Q120: is_complete=true + message final cod√© en dur
// - Logs d√©taill√©s pour d√©bogage
// - Fallback neutre: [AUTO_CONTINUE] (pas de "...")

export const runtime = "nodejs";

import { NextResponse } from "next/server";
import OpenAI from "openai";
import { createClient, SupabaseClient } from "@supabase/supabase-js";
import { buildSystemPrompt } from "../../../lib/prompts/system";
import { storeInShort } from "../../../lib/memory";
import { getBigFiveQuestion } from "../../../lib/prompts/big_five";
import { analyzeImage } from "../../../lib/photo";

// ------------ Types ------------
type ImagePart = { type: "image_url"; image_url: { url: string } };
type TextPart = { type: "text"; text: string };
type ChatContentPart = ImagePart | TextPart;
type Msg = OpenAI.Chat.Completions.ChatCompletionMessageParam;

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

// ------------ Helpers ------------
function detectStartIntent(text: string | undefined): boolean {
  if (!text) return false;
  const t = text.toLowerCase();
  return !!t.match(
    /(let'?s\s*(go|begin|start)|\b(start|begin|go)\b|c['‚Äô]est parti|on commence|commen[cs]ons|je suis (pr[e√©]t|ready)|lance|d[e√©]marre|d[e√©]butons)/
  );
}

function hasTriggerOrchestrator(s: string | undefined): boolean {
  if (!s) return false;
  return /"trigger_orchestrator"\s*:\s*true/.test(s);
}

function stripTechnicalBlocks(text: string): string {
  return (text ?? "")
    .replace(/```json[\s\S]*?```/g, "")
    .replace(/\{\s*"trigger_orchestrator"\s*:\s*true\s*\}/g, "")
    .replace(/\{\s*"score"\s*:\s*[1-5]\s*\}/g, "")
    .replace(/\[\[\s*SCORE\s*=\s*[1-5]\s*\]\]/g, "")
    .trim();
}

function extractScore(s: string): number | null {
  if (!s) return null;
  const m = s.match(/\[\[\s*SCORE\s*=\s*([1-5])\s*\]\]|["']?score["']?\s*:\s*([1-5])/i);
  if (!m) return null;
  const n = parseInt((m[1] || m[2]) as string, 10);
  return Number.isInteger(n) && n >= 1 && n <= 5 ? n : null;
}

async function callOpenAI(messages: Msg[], model = "gpt-4", temperature = 0.2) {
  const completion = await openai.chat.completions.create({ model, messages, temperature });
  return completion.choices?.[0]?.message?.content?.trim() || "";
}

async function ensureBigFiveRow(supabase: SupabaseClient, userId: string) {
  const { data } = await supabase.from("big_five").select("user_id").eq("user_id", userId).maybeSingle();
  if (!data) {
    console.log("‚ûï Cr√©ation big_five row pour", userId);
    await supabase.from("big_five").insert({ user_id: userId });
  }
}

async function getBigFiveRow(supabase: SupabaseClient, userId: string) {
  const { data } = await supabase.from("big_five").select("*").eq("user_id", userId).maybeSingle();
  return data as any | null;
}

function findCurrentIndex(row: any): number {
  // Premi√®re case qN encore vide ‚Üí c‚Äôest la question active
  for (let i = 1; i <= 120; i++) {
    if (row?.[`q${i}`] == null) return i;
  }
  return 121; // tout rempli
}

async function getLastStimulus(supabase: SupabaseClient, userId: string): Promise<string | null> {
  const { data, error } = await supabase
    .from("memories")
    .select("role, content")
    .eq("user_id", userId)
    .eq("layer", "short")
    .order("created_at", { ascending: false })
    .limit(24);
  if (error || !data) return null;
  for (const m of data) {
    if (m.role === "assistant" && typeof m.content === "string" && m.content.startsWith("[STIMULUS]")) {
      const parts = m.content.split("\n");
      parts.shift();
      const stim = parts.join("\n").trim();
      if (stim) return stim;
    }
  }
  return null;
}

async function getRecentShortMemoriesAsChat(
  supabase: SupabaseClient,
  userId: string,
  limit = 8
): Promise<Msg[]> {
  const { data, error } = await supabase
    .from("memories")
    .select("role, content, created_at")
    .eq("user_id", userId)
    .eq("layer", "short")
    .order("created_at", { ascending: false })
    .limit(limit);
  if (error || !data) return [];
  // On renvoie du plus ancien au plus r√©cent pour le chat
  return data
    .slice()
    .reverse()
    .map((m) => ({ role: m.role === "user" ? "user" : "assistant", content: m.content })) as Msg[];
}

// ------------ Route ------------
export async function POST(req: Request) {
  try {
    console.log("üîê Auth...");
    const authHeader = req.headers.get("authorization") || req.headers.get("Authorization");
    const token = authHeader?.startsWith("Bearer ") ? authHeader.slice(7) : null;
    if (!token) return NextResponse.json({ error: "Unauthorized: no token" }, { status: 401 });

    const supabase = createClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!,
      { auth: { persistSession: false, autoRefreshToken: false } }
    );

    const { data: authData, error: authErr } = await supabase.auth.getUser(token);
    if (authErr || !authData?.user?.id) {
      return NextResponse.json({ error: "Unauthorized: invalid token" }, { status: 401 });
    }
    const userId = authData.user.id;
    console.log("‚úÖ User:", userId);

    await ensureBigFiveRow(supabase, userId);
    const bfRowAtStart = await getBigFiveRow(supabase, userId);
    const isComplete = bfRowAtStart?.is_complete === true;

    // ---- Parse body ----
    const body = await req.json();
    const rawMessage: string | undefined = typeof body.message === "string" ? body.message.trim() : undefined;
    const assistantMessageFromClient: string | undefined =
      typeof body.assistant_message === "string" ? body.assistant_message.trim() : undefined;
    const rawContent: ChatContentPart[] | undefined = Array.isArray(body.content) ? body.content : undefined;

    // Compose user input (texte + image d√©crite)
    let composedUserInput = "";
    const hasImage = !!rawContent?.some((p) => p.type === "image_url" && p.image_url?.url);
    const firstImageUrl = rawContent?.find((p): p is ImagePart => p.type === "image_url")?.image_url?.url;

    if (hasImage && firstImageUrl) {
      console.log("üñºÔ∏è Image d√©tect√©e, analyse...");
      const description = await analyzeImage(firstImageUrl);
      composedUserInput = rawMessage ? `${rawMessage}\n\n[Image d√©crite]\n${description}` : description;
    } else {
      composedUserInput = rawMessage ?? (rawContent as TextPart[])?.map((p) => p.text).join("\n") ?? "";
    }

    console.log("üí¨ Input user:", composedUserInput || "(vide)");
    // Log assistant_message s‚Äôil est envoy√© par le client (optionnel)
    if (assistantMessageFromClient) console.log("ü§ù assistant_message (client):", assistantMessageFromClient);

    // ---- Stocker le message user si pr√©sent ----
    if (composedUserInput) {
      await storeInShort(supabase, userId, "user", composedUserInput);
    }

    // ---- Phase: COMPLETE (test d√©j√† fini) ----
    if (isComplete) {
      console.log("üõë Test d√©j√† complet ‚Üí phase 'complete'");
      const sys = buildSystemPrompt({ phase: "complete" });
      const history = await getRecentShortMemoriesAsChat(supabase, userId, 10);
      const messages: Msg[] = [
        { role: "system", content: sys },
        ...history,
        { role: "user", content: composedUserInput || "[AUTO_CONTINUE]" }
      ];
      const reply = await callOpenAI(messages, "gpt-4", 0.2);
      const cleaned = stripTechnicalBlocks(reply);
      await storeInShort(supabase, userId, "assistant", cleaned || reply);
      return NextResponse.json({ message: cleaned || reply });
    }

    // ---- D√©tection d'un trigger envoy√© depuis le client (si frontend passe assistant_message) ----
    if (assistantMessageFromClient && hasTriggerOrchestrator(assistantMessageFromClient)) {
      console.log("üöÄ Trigger orchestrator (depuis assistant_message client)");
      const enthusiasm = stripTechnicalBlocks(assistantMessageFromClient);
      if (enthusiasm) await storeInShort(supabase, userId, "assistant", enthusiasm);

      // QN actuelle = premi√®re colonne vide
      const bfRow = await getBigFiveRow(supabase, userId);
      let step = findCurrentIndex(bfRow);
      if (step > 120) step = 1;

      const q = getBigFiveQuestion(step);
      const stimulus = q.text;
      await storeInShort(supabase, userId, "assistant", `[STIMULUS]\n${stimulus}`);

      const sys = buildSystemPrompt({ phase: "big_five", stimulus });

      // R√©cup√©ration de toute la m√©moire courte (user + assistant), sans limite
      const { data: shortAll } = await supabase
        .from("memories")
        .select("role, content, created_at")
        .eq("user_id", userId)
        .eq("layer", "short")
        .order("created_at", { ascending: true });

      const historyAll: Msg[] =
        (shortAll ?? []).map((m) => ({ role: m.role === "user" ? "user" : "assistant", content: m.content }));

      const messages: Msg[] = [{ role: "system", content: sys }, ...historyAll];

      console.log("ü§ñ GPT start Q", step);
      const firstQ = await callOpenAI(messages, "gpt-4", 0.2);
      await storeInShort(supabase, userId, "assistant", stripTechnicalBlocks(firstQ) || firstQ);

      // Afficher enthousiasme + question dans la m√™me r√©ponse (le front affiche tout)
      const out = (enthusiasm ? enthusiasm + "\n\n" : "") + (stripTechnicalBlocks(firstQ) || firstQ);
      return NextResponse.json({ message: out });
    }

    // ---- D√©tection d'intention de commencer (via prompt intro combin√©) ----
    if (detectStartIntent(composedUserInput)) {
      console.log("‚úÖ Intent de d√©marrer d√©tect√© ‚Üí phase 'intro' (confirmation int√©gr√©e)");
      const introSys = buildSystemPrompt({ phase: "intro" });
      const introMessages: Msg[] = [{ role: "system", content: introSys }, { role: "user", content: composedUserInput }];
      const introReply = await callOpenAI(introMessages, "gpt-4", 0.0);
      console.log("üí¨ INTRO reply:", introReply);
      const enthusiasm = stripTechnicalBlocks(introReply);
      if (enthusiasm) await storeInShort(supabase, userId, "assistant", enthusiasm);

      // S'il y a trigger ‚Üí on encha√Æne imm√©diatement sur QN (souvent Q1)
      if (hasTriggerOrchestrator(introReply)) {
        const bfRow = await getBigFiveRow(supabase, userId);
        let step = findCurrentIndex(bfRow);
        if (step > 120) step = 1;

        const q = getBigFiveQuestion(step);
        const stimulus = q.text;
        console.log(`üìå Stimulus q${step}:`, stimulus);
        await storeInShort(supabase, userId, "assistant", `[STIMULUS]\n${stimulus}`);

        const sys = buildSystemPrompt({ phase: "big_five", stimulus });

        // R√©cup√©ration de toute la m√©moire courte (user + assistant), sans limite
        const { data: shortAll } = await supabase
          .from("memories")
          .select("role, content, created_at")
          .eq("user_id", userId)
          .eq("layer", "short")
          .order("created_at", { ascending: true });

        const historyAll: Msg[] =
          (shortAll ?? []).map((m) => ({ role: m.role === "user" ? "user" : "assistant", content: m.content }));

        const messages: Msg[] = [{ role: "system", content: sys }, ...historyAll];

        console.log("ü§ñ GPT start Q", step);
        const firstQ = await callOpenAI(messages, "gpt-4", 0.2);
        const firstQClean = stripTechnicalBlocks(firstQ);
        await storeInShort(supabase, userId, "assistant", firstQClean || firstQ);

        // Retourner l'enthousiasme + la 1√®re question
        const out = (enthusiasm ? enthusiasm + "\n\n" : "") + (firstQClean || firstQ);
        return NextResponse.json({ message: out });
      }

      // Pas de trigger ‚Üí renvoyer simplement le message g√©n√©r√©
      return NextResponse.json({ message: enthusiasm || introReply });
    }

    // ---- Phase Big Five en cours ? (y a-t-il un stimulus en attente) ----
    const pendingStimulus = await getLastStimulus(supabase, userId);
    if (pendingStimulus) {
      console.log("üß≠ Stimulus en cours d√©tect√© ‚Üí phase 'big_five'");
      const sys = buildSystemPrompt({ phase: "big_five", stimulus: pendingStimulus });
      // Conserver un peu d'historique court pour permettre les relances / nuance
      const history = await getRecentShortMemoriesAsChat(supabase, userId, 8);
      const messages: Msg[] = [
        { role: "system", content: sys },
        ...history,
        { role: "user", content: composedUserInput || "[AUTO_CONTINUE]" }
      ];

      const assistant = await callOpenAI(messages, "gpt-4", 0.2);
      console.log("üí¨ BIG5 assistant:", assistant);
      const score = extractScore(assistant);
      const visibleAssistant = stripTechnicalBlocks(assistant);
      if (visibleAssistant) await storeInShort(supabase, userId, "assistant", visibleAssistant);

      // Si pas de score ‚Üí l'IA est encore en train d'explorer (relance 1-2 fois)
      if (score == null) {
        console.log("‚ÑπÔ∏è Pas de SCORE d√©tect√© ‚Üí on renvoie la r√©ponse telle quelle");
        return NextResponse.json({ message: visibleAssistant || assistant });
      }

      console.log("üß† SCORE d√©tect√©:", score);

      // √âcrire score (avec inversion si n√©cessaire) dans la bonne colonne qN (premi√®re vide)
      const row = await getBigFiveRow(supabase, userId);
      if (!row) {
        console.error("‚ùå big_five row introuvable pour user:", userId);
        return NextResponse.json({ error: "big_five missing" }, { status: 500 });
      }

      const currentIndex = findCurrentIndex(row); // c'est bien la qN qu'on vient de scorer
      const qMeta = getBigFiveQuestion(currentIndex);
      const col = qMeta.key || `q${currentIndex}`;
      const finalScore = qMeta.isReversed ? 6 - score : score;

      console.log(`üìù √âcriture ${col} = ${finalScore} (isReversed=${qMeta.isReversed})`);
      await supabase.from("big_five").update({ [col]: finalScore }).eq("user_id", userId);

      // Fin de test si Q120 vient d'√™tre remplie
      if (currentIndex === 120) {
        console.log("üèÅ Q120 atteinte ‚Üí is_complete = true");
        await supabase.from("big_five").update({ is_complete: true }).eq("user_id", userId);
        const doneMsg = "‚úÖ Test termin√©. Merci pour ta participation.";
        await storeInShort(supabase, userId, "assistant", doneMsg);
        return NextResponse.json({ message: doneMsg });
      }

      // Sinon, pr√©parer Qn+1 et la poser imm√©diatement
      const nextIndex = currentIndex + 1;
      const nextMeta = getBigFiveQuestion(nextIndex);
      const nextStimulus = nextMeta.text;
      console.log(`‚û°Ô∏è Pr√©paration stimulus q${nextIndex}`);
      await storeInShort(supabase, userId, "assistant", `[STIMULUS]\n${nextStimulus}`);

      const nextSys = buildSystemPrompt({ phase: "big_five", stimulus: nextStimulus });
      const nextHistory = await getRecentShortMemoriesAsChat(supabase, userId, 8);
      const nextMessages: Msg[] = [
        { role: "system", content: nextSys },
        ...nextHistory,
        { role: "user", content: "[AUTO_CONTINUE]" }
      ];
      console.log("üì§ GPT ‚Üí Q", nextIndex);
      const nextQ = await callOpenAI(nextMessages, "gpt-4", 0.2);
      const nextQClean = stripTechnicalBlocks(nextQ);
      if (nextQClean) await storeInShort(supabase, userId, "assistant", nextQClean);
      return NextResponse.json({ message: nextQClean || nextQ });
    }

    // ---- Phase INTRO par d√©faut ----
    console.log("üëã Phase 'intro'");
    const sys = buildSystemPrompt({ phase: "intro" });
    const history = await getRecentShortMemoriesAsChat(supabase, userId, 6);
    const messages: Msg[] = [
      { role: "system", content: sys },
      ...history,
      { role: "user", content: composedUserInput || "[AUTO_CONTINUE]" }
    ];
    const reply = await callOpenAI(messages, "gpt-4", 0.2);

    // üîó Nouveau : si l'intro combin√©e produit un trigger, on encha√Æne Q1 ici aussi
    if (hasTriggerOrchestrator(reply)) {
      const enthusiasm = stripTechnicalBlocks(reply);
      if (enthusiasm) await storeInShort(supabase, userId, "assistant", enthusiasm);

      const bfRow = await getBigFiveRow(supabase, userId);
      let step = findCurrentIndex(bfRow);
      if (step > 120) step = 1;

      const q = getBigFiveQuestion(step);
      const stimulus = q.text;
      console.log(`üìå Stimulus q${step}:`, stimulus);
      await storeInShort(supabase, userId, "assistant", `[STIMULUS]\n${stimulus}`);

      const qSys = buildSystemPrompt({ phase: "big_five", stimulus });

      // R√©cup√©ration de toute la m√©moire courte (user + assistant), sans limite
      const { data: shortAll } = await supabase
        .from("memories")
        .select("role, content, created_at")
        .eq("user_id", userId)
        .eq("layer", "short")
        .order("created_at", { ascending: true });

      const historyAll: Msg[] =
        (shortAll ?? []).map((m) => ({ role: m.role === "user" ? "user" : "assistant", content: m.content }));

      const qMessages: Msg[] = [{ role: "system", content: qSys }, ...historyAll];

      console.log("ü§ñ GPT start Q", step);
      const firstQ = await callOpenAI(qMessages, "gpt-4", 0.2);
      const firstQClean = stripTechnicalBlocks(firstQ);
      await storeInShort(supabase, userId, "assistant", firstQClean || firstQ);

      const out = (enthusiasm ? enthusiasm + "\n\n" : "") + (firstQClean || firstQ);
      return NextResponse.json({ message: out });
    }

    const cleaned = stripTechnicalBlocks(reply);
    await storeInShort(supabase, userId, "assistant", cleaned || reply);
    return NextResponse.json({ message: cleaned || reply });
  } catch (err) {
    console.error("‚ùå Error in ask route:", err);
    return NextResponse.json({ error: "Server error" }, { status: 500 });
  }
}
